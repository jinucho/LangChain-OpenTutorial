{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25733da0",
   "metadata": {},
   "source": [
    "# Elasticsearch\n",
    "\n",
    "- Author: [liniar](https://github.com/namyoungkim)\n",
    "- Peer Review: [Joseph](https://github.com/XaviereKU), [Sohyeon Yim](https://github.com/sohyunwriter), [BokyungisaGod](https://github.com/BokyungisaGod)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/09-VectorStore/your-notebook-file-name) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/09-VectorStore/your-notebook-file-name)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers how to use **Elasticsearch** with **LangChain** .\n",
    "\n",
    "This tutorial walks you through using **CRUD** operations with the **Elasticsearch** **storing** , **updating** , **deleting** documents, and performing **similarity-based retrieval** .\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [What is Elasticsearch?](#what-is-Elasticsearch?)\n",
    "- [Prepare Data](#Prepare-Data)\n",
    "- [Setting up Elasticsearch](#Setting-up-Elasticsearch)\n",
    "- [Document Manager](#document-manager)\n",
    "\n",
    "\n",
    "### References\n",
    "- [Elasticsearch Official Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/index.html)  \n",
    "- [Elasticsearch Vector Search Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html)  \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fac085",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- ```langchain-opentutorial``` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98da7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800c732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain-core\",\n",
    "        \"langchain_openai\",\n",
    "        \"elasticsearch\",\n",
    "        \"python-dotenv\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b36bafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"Your OPENAI API KEY\",\n",
    "        \"LANGCHAIN_API_KEY\": \"Your LangChain API KEY\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Elasticsearch\",\n",
    "        \"ES_URL\": \"Your Elasticsearch URI\",\n",
    "        \"ES_API_KEY\": \"Your Elasticsearch API KEY\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8011a0c7",
   "metadata": {},
   "source": [
    "You can alternatively set API keys such as ```OPENAI_API_KEY``` in a ```.env``` file and load them.\n",
    "\n",
    "[Note] This is not necessary if you've already set the required API keys in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d7e764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6890920d",
   "metadata": {},
   "source": [
    "### Setup Elasticsearch\n",
    "In order to use the **Elasticsearch** vector search you must install the langchain-elasticsearch package.\n",
    "\n",
    " 🚀 Setting Up Elasticsearch with Elastic Cloud (Colab Compatible)\n",
    "- Elastic Cloud allows you to manage **Elasticsearch** seamlessly in the cloud, eliminating the need for local installations.\n",
    "- It integrates well with Google Colab, enabling efficient experimentation and prototyping.\n",
    "\n",
    "\n",
    " 📚 What is Elastic Cloud?  \n",
    "- **Elastic Cloud** is a managed **Elasticsearch** service provided by Elastic.  \n",
    "- Supports **custom cluster configurations** and **auto-scaling** . \n",
    "- Deployable on **AWS** , **GCP** , and **Azure** .  \n",
    "- Compatible with **Google Colab,** allowing simplified cloud-based workflows.  \n",
    "\n",
    " 📌 Getting Started with Elastic Cloud  \n",
    "1. **Sign up for Elastic Cloud’s Free Trial** .  \n",
    "    - [Free Trial](https://cloud.elastic.co/registration?utm_source=langchain&utm_content=documentation)\n",
    "2. Create an Elasticsearch **Cluster** .\n",
    "3. Retrieve your **Elasticsearch URL** and **Elasticsearch API Key** from the Elastic Cloud Console.  \n",
    "4. Add the following to your `.env` file\n",
    "    ```\n",
    "    ES_URL=https://my-elasticsearch-project-abd...:123\n",
    "    ES_API_KEY=bk9X...\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebf6496",
   "metadata": {},
   "source": [
    "## What is Elasticsearch?\n",
    "**Elasticsearch** is an open-source, distributed search and analytics engine designed to store, search, and analyze both structured and unstructured data in real-time.\n",
    "\n",
    "### Key Features  \n",
    "- **Real-Time Search:** Instantly searchable data upon ingestion  \n",
    "- **Large-Scale Data Processing:** Efficient handling of vast datasets  \n",
    "- **Scalability:** Flexible scaling through clustering and distributed architecture  \n",
    "- **Versatile Search Support:** Keyword search, semantic search, and multimodal search  \n",
    "\n",
    "### Use Cases  \n",
    "- **Log Analytics:** Real-time monitoring of system and application logs  \n",
    "- **Monitoring:** Server and network health tracking  \n",
    "- **Product Recommendations:** Behavior-based recommendation systems  \n",
    "- **Natural Language Processing (NLP):** Semantic text searches  \n",
    "- **Multimodal Search:** Text-to-image and image-to-image searches  \n",
    "\n",
    "### Vector Database Functionality in Elasticsearch  \n",
    "- **Elasticsearch** supports vector data storage and similarity search via **Dense Vector Fields** . As a vector database, it excels in applications like NLP, image search, and recommendation systems.\n",
    "\n",
    "### Core Vector Database Features  \n",
    "- **Dense Vector Field:** Store and query high-dimensional vectors  \n",
    "- **KNN (k-Nearest Neighbors) Search:** Find vectors most similar to the input  \n",
    "- **Semantic Search:** Perform meaning-based searches beyond keyword matching  \n",
    "- **Multimodal Search:** Combine text and image data for advanced search capabilities  \n",
    "\n",
    "### Vector Search Use Cases  \n",
    "- **Semantic Search:** Understand user intent and deliver precise results  \n",
    "- **Text-to-Image Search:** Retrieve relevant images from textual descriptions  \n",
    "- **Image-to-Image Search:** Find visually similar images in a dataset  \n",
    "\n",
    "**Elasticsearch** goes beyond traditional text search engines, offering robust vector database capabilities essential for NLP and multimodal search applications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b5bd2",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "This section guides you through the **data preparation process** .\n",
    "\n",
    "This section includes the following components:\n",
    "\n",
    "- Data Introduction\n",
    "\n",
    "- Preprocess Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508ae7f7",
   "metadata": {},
   "source": [
    "### Data Introduction\n",
    "\n",
    "In this tutorial, we will use the fairy tale **📗 The Little Prince** in PDF format as our data.\n",
    "\n",
    "This material complies with the **Apache 2.0 license** .\n",
    "\n",
    "The data is used in a text (.txt) format converted from the original PDF.\n",
    "\n",
    "You can view the data at the link below.\n",
    "- [Data Link](https://huggingface.co/datasets/sohyunwriter/the_little_prince)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ea4f4",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "\n",
    "In this tutorial section, we will preprocess the text data from The Little Prince and convert it into a list of ```LangChain Document``` objects with metadata. \n",
    "\n",
    "Each document chunk will include a ```title``` field in the metadata, extracted from the first line of each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e4cac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def preprocessing_data(content:str)->List[Document]:\n",
    "    # 1. Split the text by double newlines to separate sections\n",
    "    blocks = content.split(\"\\n\\n\")\n",
    "\n",
    "    # 2. Initialize the text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,              # Maximum number of characters per chunk\n",
    "        chunk_overlap=50,            # Overlap between chunks to preserve context\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"]  # Order of priority for splitting\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    # 3. Loop through each section\n",
    "    for block in blocks:\n",
    "        lines = block.strip().splitlines()\n",
    "        if not lines:\n",
    "            continue\n",
    "\n",
    "        # Extract title from the first line using square brackets [ ]\n",
    "        first_line = lines[0]\n",
    "        title_match = re.search(r\"\\[(.*?)\\]\", first_line)\n",
    "        title = title_match.group(1).strip() if title_match else \"\"\n",
    "\n",
    "        # Remove the title line from content\n",
    "        body = \"\\n\".join(lines[1:]).strip()\n",
    "        if not body:\n",
    "            continue\n",
    "\n",
    "        # 4. Chunk the section using the text splitter\n",
    "        chunks = text_splitter.split_text(body)\n",
    "\n",
    "        # 5. Create a LangChain Document for each chunk with the same title metadata\n",
    "        for chunk in chunks:\n",
    "            documents.append(Document(page_content=chunk, metadata={\"title\": title}))\n",
    "\n",
    "    print(f\"Generated {len(documents)} chunked documents.\")\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d091a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 262 chunked documents.\n"
     ]
    }
   ],
   "source": [
    "# Load the entire text file\n",
    "with open(\"./data/the_little_prince.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Preprocess Data\n",
    "docs = preprocessing_data(content=content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1977d4ff",
   "metadata": {},
   "source": [
    "## Setting up Elasticsearch\n",
    "\n",
    "This part walks you through the initial setup of **Elasticsearch** .\n",
    "\n",
    "This section includes the following components:\n",
    "\n",
    "- Load Embedding Model\n",
    "\n",
    "- Load Elasticsearch Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee56b2",
   "metadata": {},
   "source": [
    "### Load Embedding Model\n",
    "\n",
    "In this section, you'll learn how to load an embedding model.\n",
    "\n",
    "This tutorial uses **OpenAI's** **API-Key** for loading the model.\n",
    "\n",
    "*💡 If you prefer to use another embedding model, see the instructions below.*\n",
    "- [Embedding Models](https://python.langchain.com/docs/integrations/text_embedding/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bd5c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f65795",
   "metadata": {},
   "source": [
    "### Load Elasticsearch Client\n",
    "\n",
    "In this section, we'll show you how to load the **database client object** using the **Python SDK** for **Elasticsearch** .\n",
    "- [Elasticsearch Python SDK Docs](https://www.elastic.co/docs/reference/elasticsearch/clients/python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d97822c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from elasticsearch import Elasticsearch, exceptions as es_exceptions\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_db_client(\n",
    "    es_url: str = None,\n",
    "    api_key: str = None,\n",
    "    timeout: int = 120,\n",
    "    retry_on_timeout: bool = True\n",
    ") -> Elasticsearch:\n",
    "    \"\"\"\n",
    "    Initializes and returns an Elasticsearch client instance.\n",
    "    \n",
    "    This function loads configuration (e.g., API key, host) from environment\n",
    "    variables or default values and creates a client object to interact\n",
    "    with the Elasticsearch Python SDK.\n",
    "\n",
    "    Args:\n",
    "        es_url (str): Elasticsearch URL. If None, uses 'ES_URL' env var.\n",
    "        api_key (str): API key. If None, uses 'ES_API_KEY' env var.\n",
    "        timeout (int): Request timeout in seconds.\n",
    "        retry_on_timeout (bool): Whether to retry on timeout.\n",
    "\n",
    "    Returns:\n",
    "        Elasticsearch: An instance of the Elasticsearch client.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If required configuration is missing.\n",
    "        es_exceptions.ConnectionError: If connection fails.\n",
    "    \"\"\"\n",
    "    es_url = es_url or os.getenv(\"ES_URL\")\n",
    "    api_key = api_key or os.getenv(\"ES_API_KEY\")\n",
    "    if not es_url or not api_key:\n",
    "        raise ValueError(\"Elasticsearch URL and API key must be provided.\")\n",
    "\n",
    "    client = Elasticsearch(\n",
    "        es_url, api_key=api_key, request_timeout=timeout, retry_on_timeout=retry_on_timeout\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        if client.ping():\n",
    "            logger.info(\"✅ Successfully connected to Elasticsearch!\")\n",
    "        else:\n",
    "            logger.error(\"❌ Failed to connect to Elasticsearch (ping returned False).\")\n",
    "            raise es_exceptions.ConnectionError(\"Failed to connect to Elasticsearch.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Elasticsearch connection error: {e}\")\n",
    "        raise\n",
    "\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5f4116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:HEAD https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/ [status:200 duration:0.603s]\n",
      "INFO:__main__:✅ Successfully connected to Elasticsearch!\n"
     ]
    }
   ],
   "source": [
    "client = get_db_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541b36a",
   "metadata": {},
   "source": [
    "### Create Index\n",
    "If you are successfully connected to **Elasticsearch**, some basic indexes are already created.\n",
    "\n",
    "But, in this tutorial we will create a new index with ```ElasticsearchIndexManager``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb187876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:elastic_transport.transport:HEAD https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:200 duration:0.194s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Index 'langchain_tutorial_es' already exists. Skipping creation.\n",
      "{'status': 'exists', 'index_name': 'langchain_tutorial_es', 'embedding_dims': 3072, 'metric': 'cosine'}\n"
     ]
    }
   ],
   "source": [
    "from utils.elasticsearch import ElasticsearchIndexManager\n",
    "\n",
    "#  Create IndexManager Object\n",
    "index_manger = ElasticsearchIndexManager(client)\n",
    "\n",
    "# Create A New Index\n",
    "index_name = \"langchain_tutorial_es\"\n",
    "\n",
    "tutorial_index=index_manger.create_index(\n",
    "    embedding, index_name=index_name, metric=\"cosine\"\n",
    ")\n",
    "\n",
    "print(tutorial_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b97fdd",
   "metadata": {},
   "source": [
    "### Delete Index\n",
    "If you want to remove an existing index from **Elasticsearch**, you can use the `ElasticsearchIndexManager` class to delete it easily.\n",
    "\n",
    "This is useful when you want to reset your data or clean up unused indexes during development or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "587a493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:HEAD https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:200 duration:0.193s]\n",
      "INFO:elastic_transport.transport:DELETE https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:200 duration:0.227s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"✅ Index 'langchain_tutorial_es' deleted successfully.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete A New Index\n",
    "index_manger.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99385c90",
   "metadata": {},
   "source": [
    "To proceed with the tutorial, let’s create the index once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ae0c3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:elastic_transport.transport:HEAD https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:404 duration:0.195s]\n",
      "INFO:elastic_transport.transport:PUT https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:200 duration:0.284s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Index 'langchain_tutorial_es' created successfully.\n",
      "{'status': 'created', 'index_name': 'langchain_tutorial_es', 'embedding_dims': 3072, 'metric': 'cosine'}\n"
     ]
    }
   ],
   "source": [
    "tutorial_index=index_manger.create_index(\n",
    "    embedding, index_name=index_name, metric=\"cosine\"\n",
    ")\n",
    "\n",
    "print(tutorial_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a97a0",
   "metadata": {},
   "source": [
    "## Document Manager\n",
    "\n",
    "For the **LangChain-OpenTutorial**, we have implemented a custom set of **CRUD** functionalities for VectorDBs\n",
    "\n",
    "The following operations are included:\n",
    "\n",
    "- ```upsert``` : Update existing documents or insert if they don’t exist\n",
    "\n",
    "- ```upsert_parallel``` : Perform upserts in parallel for large-scale data\n",
    "\n",
    "- ```similarity_search``` : Search for similar documents based on embeddings\n",
    "\n",
    "- ```delete``` : Remove documents based on filter conditions\n",
    "\n",
    "Each of these features is implemented as class methods specific to each VectorDB.\n",
    "\n",
    "In this tutorial, you'll learn how to use these methods to interact with your VectorDB.\n",
    "\n",
    "*We plan to continuously expand the functionality by adding more common operations in the future.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a40601",
   "metadata": {},
   "source": [
    "### Create Instance\n",
    "\n",
    "First, create an instance of the Elasticsearch helper class to use its CRUD functionalities.\n",
    "\n",
    "This class is initialized with the **Elasticsearch Python SDK client instance** and the **embedding model instance** , both of which were defined in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dccab807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ElasticsearchDocumentManager\n",
    "from utils.elasticsearch import ElasticsearchDocumentManager\n",
    "\n",
    "# connect to tutorial_index\n",
    "crud_manager = ElasticsearchDocumentManager(\n",
    "    client=client, index_name=index_name, embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0c67f",
   "metadata": {},
   "source": [
    "Now you can use the following **CRUD** operations with the ```crud_manager``` instance.\n",
    "\n",
    "These instance allow you to easily manage documents in your **Elasticsearch** ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c53c5",
   "metadata": {},
   "source": [
    "### Upsert Document\n",
    "\n",
    "**Update** existing documents or **insert** if they don’t exist\n",
    "\n",
    "**✅ Args**\n",
    "\n",
    "- ```texts``` : Iterable[str] – List of text contents to be inserted/updated.\n",
    "\n",
    "- ```metadatas``` : Optional[List[Dict]] – List of metadata dictionaries for each text (optional).\n",
    "\n",
    "- ```ids``` : Optional[List[str]] – Custom IDs for the documents. If not provided, IDs will be auto-generated.\n",
    "\n",
    "- ```**kwargs``` : Extra arguments for the underlying vector store.\n",
    "\n",
    "**🔄 Return**\n",
    "\n",
    "- None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6c32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:elastic_transport.transport:PUT https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:0.838s]\n",
      "INFO:utils.elasticsearch:✅ Bulk upsert completed successfully.\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "# Create ID for each document\n",
    "ids = [str(uuid4()) for _ in docs]\n",
    "\n",
    "args = {\n",
    "    \"texts\": [doc.page_content for doc in docs[:2]],\n",
    "    \"metadatas\": [doc.metadata for doc in docs[:2]],\n",
    "    \"ids\": ids[:2],\n",
    "    # Add additional parameters if you need\n",
    "}\n",
    "\n",
    "crud_manager.upsert(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278fe1ed",
   "metadata": {},
   "source": [
    "### Upsert Parallel\n",
    "\n",
    "Perform **upserts** in **parallel** for large-scale data\n",
    "\n",
    "**✅ Args**\n",
    "\n",
    "- ```texts``` : Iterable[str] – List of text contents to be inserted/updated.\n",
    "\n",
    "- ```metadatas``` : Optional[List[Dict]] – List of metadata dictionaries for each text (optional).\n",
    "\n",
    "- ```ids``` : Optional[List[str]] – Custom IDs for the documents. If not provided, IDs will be auto-generated.\n",
    "\n",
    "- ```batch_size``` : int – Number of documents per batch (default: 32).\n",
    "\n",
    "- ```workers``` : int – Number of parallel workers (default: 10).\n",
    "\n",
    "- ```**kwargs``` : Extra arguments for the underlying vector store.\n",
    "\n",
    "**🔄 Return**\n",
    "\n",
    "- None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89dd8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:elastic_transport.transport:PUT https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.858s]\n",
      "INFO:elastic_transport.transport:PUT https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:3.299s]\n",
      "INFO:elastic_transport.transport:PUT https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:3.570s]\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "args = {\n",
    "    \"texts\": [doc.page_content for doc in docs],\n",
    "    \"metadatas\": [doc.metadata for doc in docs],\n",
    "    \"ids\": ids,\n",
    "    # Add additional parameters if you need\n",
    "}\n",
    "\n",
    "crud_manager.upsert_parallel(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beea197",
   "metadata": {},
   "source": [
    "### Similarity Search\n",
    "\n",
    "Search for **similar documents** based on **embeddings** .\n",
    "\n",
    "This method uses **\"cosine similarity\"** .\n",
    "\n",
    "\n",
    "**✅ Args**\n",
    "\n",
    "- ```query``` : str – The text query for similarity search.\n",
    "\n",
    "- ```k``` : int – Number of top results to return (default: 10).\n",
    "\n",
    "- ```**kwargs``` : Additional search options (e.g., filters).\n",
    "\n",
    "**🔄 Return**\n",
    "\n",
    "- ```results``` : List[Document] – A list of LangChain Document objects ranked by similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5859782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:elastic_transport.transport:POST https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search [status:200 duration:0.954s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Rank 1 | Title : Chapter 21\n",
      "Contents : And he went back to meet the fox. \n",
      "\"Goodbye,\" he said. \n",
      "\"Goodbye,\" said the fox. \"And now here is my secret, a very simple secret: It is only with the heart that one can see rightly; what is essential is invisible to the eye.\" \n",
      "\"What is essential is invisible to the eye,\" the little prince repeated, so that he would be sure to remember.\n",
      "\"It is the time you have wasted for your rose that makes your rose so important.\"\n",
      "Similarity Score : 0.7546974\n",
      "\n",
      "====================================================================================================\n",
      "Rank 2 | Title : Chapter 24\n",
      "Contents : \"Yes,\" I said to the little prince. \"The house, the stars, the desert-- what gives them their beauty is something that is invisible!\" \n",
      "\"I am glad,\" he said, \"that you agree with my fox.\"\n",
      "Similarity Score : 0.7476631\n",
      "\n",
      "====================================================================================================\n",
      "Rank 3 | Title : Chapter 25\n",
      "Contents : \"The men where you live,\" said the little prince, \"raise five thousand roses in the same garden-- and they do not find in it what they are looking for.\" \n",
      "\"They do not find it,\" I replied. \n",
      "\"And yet what they are looking for could be found in one single rose, or in a little water.\" \n",
      "\"Yes, that is true,\" I said. \n",
      "And the little prince added: \n",
      "\"But the eyes are blind. One must look with the heart...\"\n",
      "Similarity Score : 0.7111699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search by Query\n",
    "results = crud_manager.search(query=\"What is essential is invisible to the eye.\", k=3)\n",
    "\n",
    "for idx,doc in enumerate(results):\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Rank {idx+1} | Title : {doc.metadata['title']}\")\n",
    "    print(f\"Contents : {doc.page_content}\")\n",
    "    print(f\"Similarity Score : {doc.metadata['score']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2577dd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:elastic_transport.transport:POST https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search [status:200 duration:0.584s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Rank 1 | Title : Chapter 4\n",
      "Contents : I have serious reason to believe that the planet from which the little prince came is the asteroid known as B-612. This asteroid has only once been seen through the telescope. That was by a Turkish astronomer, in 1909. \n",
      "(picture)\n",
      "On making his discovery, the astronomer had presented it to the International Astronomical Congress, in a great demonstration. But he was in Turkish costume, and so nobody would believe what he said.\n",
      "Grown-ups are like that...\n",
      "Similarity Score : 0.8311258\n",
      "\n",
      "====================================================================================================\n",
      "Rank 2 | Title : Chapter 4\n",
      "Contents : - the narrator speculates as to which asteroid from which the little prince came　　\n",
      "I had thus learned a second fact of great importance: this was that the planet the little prince came from was scarcely any larger than a house!\n",
      "Similarity Score : 0.81760435\n",
      "\n",
      "====================================================================================================\n",
      "Rank 3 | Title : Chapter 9\n",
      "Contents : - the little prince leaves his planet\n",
      "Similarity Score : 0.8035729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search with filters\n",
    "results = crud_manager.search(\n",
    "    query=\"Which asteroid did the little prince come from?\",\n",
    "    k=3,\n",
    "    filters={\"title\":\"Chapter 4\"}\n",
    "    )\n",
    "\n",
    "for idx,doc in enumerate(results):\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Rank {idx+1} | Title : {doc.metadata['title']}\")\n",
    "    print(f\"Contents : {doc.page_content}\")\n",
    "    print(f\"Similarity Score : {doc.metadata['score']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0ed0c",
   "metadata": {},
   "source": [
    "### Delete Document\n",
    "\n",
    "Delete documents based on filter conditions\n",
    "\n",
    "**✅ Args**\n",
    "\n",
    "- ```ids``` : Optional[List[str]] – List of document IDs to delete. If None, deletion is based on filter.\n",
    "\n",
    "- ```filters``` : Optional[Dict] – Dictionary specifying filter conditions (e.g., metadata match).\n",
    "\n",
    "- ```**kwargs``` : Any additional parameters.\n",
    "\n",
    "**🔄 Return**\n",
    "\n",
    "- None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e3a2c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search [status:200 duration:0.196s]\n",
      "INFO:elastic_transport.transport:DELETE https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/egW1sJYBLV0ipYAYto5p [status:200 duration:0.194s]\n",
      "INFO:elastic_transport.transport:DELETE https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/HgW1sJYBLV0ipYAY1I_A [status:200 duration:0.194s]\n",
      "INFO:elastic_transport.transport:POST https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search [status:200 duration:0.197s]\n",
      "INFO:elastic_transport.transport:DELETE https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/ewW1sJYBLV0ipYAYto5p [status:200 duration:0.195s]\n",
      "INFO:elastic_transport.transport:DELETE https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/HwW1sJYBLV0ipYAY1I_A [status:200 duration:0.193s]\n",
      "INFO:elastic_transport.transport:POST https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search [status:200 duration:0.193s]\n",
      "INFO:elastic_transport.transport:DELETE https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/IAW1sJYBLV0ipYAY1I_A [status:200 duration:0.193s]\n",
      "INFO:elastic_transport.transport:POST https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search [status:200 duration:0.210s]\n",
      "INFO:elastic_transport.transport:DELETE https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/IQW1sJYBLV0ipYAY1I_A [status:200 duration:0.194s]\n",
      "INFO:elastic_transport.transport:POST https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search [status:200 duration:0.194s]\n",
      "INFO:elastic_transport.transport:DELETE https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/IgW1sJYBLV0ipYAY1I_A [status:200 duration:0.193s]\n"
     ]
    }
   ],
   "source": [
    "# Delete by ids\n",
    "del_ids = ids[:5]  # The 'ids' value you want to delete\n",
    "crud_manager.delete(ids=del_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60bcb4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_delete_by_query?conflicts=proceed [status:200 duration:0.194s]\n"
     ]
    }
   ],
   "source": [
    "# Delete by ids with filters\n",
    "filters = {\"page\": 6}\n",
    "crud_manager.delete(filters={\"title\": \"chapter 6\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30d42d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://de7f5f4e2c8a452597e8e4db54c98b30.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_delete_by_query?conflicts=proceed [status:200 duration:0.291s]\n"
     ]
    }
   ],
   "source": [
    "# Delete All\n",
    "crud_manager.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-gmgjIYR5-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
